{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1fV+PXlH6fYwI1A4iKT0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arjunmukundann/Heart-Disease-Prediction/blob/main/Heart_disease_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heart Disease Prediction\n"
      ],
      "metadata": {
        "id": "4YB2GzKNvf7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this machine learning project, I have collected the dataset from Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) and I will be using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not."
      ],
      "metadata": {
        "id": "G5YJTxizvo69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "LeiWzqP7v2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's first import all the necessary libraries. I'll use numpy and pandas to start with.  For implementing Machine Learning models and processing of data, I will use the sklearn library."
      ],
      "metadata": {
        "id": "uHN1-ptXwa0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zzMKOegswfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the train_test_split method. To scale the features, I am using StandardScaler.\n"
      ],
      "metadata": {
        "id": "y_QLyBEGwkLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "_1Rwr1_lwqcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I'll import all the Machine Learning algorithms I will be using.\n",
        "\n",
        "\n",
        "\n",
        "*   K Neighbors Classifier\n",
        "*   Support Vector Classifier\n",
        "*   Decision Tree Classifier\n",
        "*   Random Forest Classifier\n"
      ],
      "metadata": {
        "id": "ZRmpBahdws9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "6kXngxf0myms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Importing dataset\n"
      ],
      "metadata": {
        "id": "0qRcq9vom0DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file dataset.csv. I'll use the pandas read_csv method to read the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "uG4OPlUcm-kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset.csv')\n"
      ],
      "metadata": {
        "id": "Kal_Z4wsnC5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is now loaded into the variable dataset. I'll just take a glimpse of the data using the info() methods before I actually start processing and visualizing it.\n",
        "\n",
        "\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "* RangeIndex: 303 entries, 0 to 302\n",
        "* Data columns (total 14 columns):\n",
        "* age         ----   303 non-null int64\n",
        "* sex   ----      303 non-null int64\n",
        "* cp        ----  303 non-null int64\n",
        "* trestbps   ----  303 non-null int64\n",
        "* chol      ----   303 non-null int64\n",
        "* fbs       ----   303 non-null int64\n",
        "* restecg    ----  303 non-null int64\n",
        "* thalach    ----  303 non-null int64\n",
        "* exang      ----  303 non-null int64\n",
        "* oldpeak    ----  303 non-null float64\n",
        "* slope       ---- 303 non-null int64\n",
        "* ca          ---- 303 non-null int64\n",
        "* thal        ---- 303 non-null int64\n",
        "* target     ----  303 non-null int64\n",
        "* dtypes: float64(1), int64(13)\n",
        "* memory usage: 33.3 KB\n",
        "\n",
        "\n",
        "\n",
        "Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of 13 features along with one target value which we wish to find."
      ],
      "metadata": {
        "id": "839hiZ_eo70M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the dataset into the Training set and Test set"
      ],
      "metadata": {
        "id": "abzwMPrSqr6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll now import train_test_split to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data."
      ],
      "metadata": {
        "id": "hyGrTbjrrZmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
      ],
      "metadata": {
        "id": "J0YnnWn5rcmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n"
      ],
      "metadata": {
        "id": "Q8QAEDZCq59J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.I will use the \"StandardScaler\" from sklearn to scale my dataset."
      ],
      "metadata": {
        "id": "hmYknu62rCyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test"
      ],
      "metadata": {
        "id": "Udd50fJKrVZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1U3A8Fkiqw8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine"
      ],
      "metadata": {
        "id": "NgeXAuvhsOd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several kernels for Support Vector Classifier. I'll test linear kernel.\n"
      ],
      "metadata": {
        "id": "q0oKVRfbszF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8DxtMRdbsX_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now l'll be implementing confusion matrix for svm after that implementing cross validation step.\n",
        "\n",
        "\n",
        "**confusion matrix**"
      ],
      "metadata": {
        "id": "BarO2qP6tVlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "4mRgtPOQuHL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[  24   ,     9  ]\n",
        "\n",
        " [   2 ,\n",
        "          41   ]  ]\n",
        "\n",
        "0.8552631578947368\n",
        "\n",
        "The above metrix indicates that,\n",
        "\n",
        "\n",
        "* True Negatives - 24\n",
        "* False Positives- \t9\n",
        "* False Negatives-  2\n",
        "* True Positives -\t41\n",
        "\n",
        "* *Key insights*:\n",
        "\n",
        "  * False Positives (9): These are cases incorrectly flagged as \"Disease\" when there is none.\n",
        "  * False Negatives (2): These are cases missed by the model (it predicts \"No Disease\" when there is actually a disease). This number is very low, indicating the model is great at identifying heart disease.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUdzhgb8uLmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**"
      ],
      "metadata": {
        "id": "BCbNGr3bxGbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n",
        "print(\"Standard Deviation:\", scores.std())\n"
      ],
      "metadata": {
        "id": "4PP7SjIsxLy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation Scores: [0.81967213 0.8852459  0.80327869 0.86666667 0.76666667]\n",
        "\n",
        "\n",
        "Mean Accuracy: 0.8283060109289618\n",
        "\n",
        "Standard Deviation: 0.042927871064461436\n",
        "\n",
        "\n",
        "1. Mean Accuracy (82.83%): This indicates consistent performance across the 5 folds.\n",
        "2. Standard Deviation (4.29%): A low standard deviation means the model's performance is stable across the different data splits.\n",
        "\n",
        "\n",
        "**Observation**\n",
        "\n",
        "* Best recall (95.3%), crucial for identifying true positives in medical diagnosis.\n",
        "* Highest cross-validation accuracy (82.83%) and lowest standard deviation (4.29%), making it the most stable and generalizable model.\n",
        "* Strong F1-Score (88.2%), close to k-NN.\n",
        "* Slightly lower precision (82%) compared to k-NN.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "   * SVM is the best overall model due to its excellent generalization, stability, and high recall, making it ideal for medical applications."
      ],
      "metadata": {
        "id": "U7xNovJ0xN2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Neighbors classifier"
      ],
      "metadata": {
        "id": "Ut_WwC2IxuFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification score varies based on different values of neighbors that we choose."
      ],
      "metadata": {
        "id": "gtll3q4Ix6rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 8, metric = 'minkowski', p = 2)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QWlmF6j7yYSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now l'll be implementing confusion matrix for KNN after that implementing cross validation step.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**confusion matrix**"
      ],
      "metadata": {
        "id": "kgCG78KrypCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "7VkVONJNyqpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[27  , 6]\n",
        "\n",
        " [ 4 ,39]]\n",
        "\n",
        "0.868421052631579\n",
        "\n",
        "The above metrix indicates that ,\n",
        "1. True Negatives -   27\n",
        "2. False Positives - \t6\n",
        "3.False Negatives - \t4\n",
        "4.True Positives   - \t39\n",
        "* *Key insights*:\n",
        " * True Negatives (27): k-NN correctly identifies more cases as \"No Disease\"compared to the SVM and Random Forest models.\n",
        "  * False Positives (6): The lowest among all models tested so far.\n",
        "  * False Negatives (4): Matches the Random Forest model, but worse than SVM .\n",
        "\n"
      ],
      "metadata": {
        "id": "E7SQSEYQzBxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qBlhRhnRyhgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**"
      ],
      "metadata": {
        "id": "-XlrDbRM0cK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n",
        "print(\"Standard Deviation:\", scores.std())\n"
      ],
      "metadata": {
        "id": "EQKUFsz70gpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation Scores: [0.57377049 0.63934426 0.62295082 0.73333333 0.53333333]\n",
        "\n",
        "Mean Accuracy: 0.6205464480874316\n",
        "\n",
        "Standard Deviation: 0.06763747153255116\n",
        "\n",
        "1. Mean Accuracy (62.05%): Significantly lower than the SVM (82.83%) and Random Forest (79.21%) models, showing that the k-NN model struggles to generalize well on different splits of the data.\n",
        "2. Standard Deviation (6.76%): Higher than SVM (4.29%), indicating less stability across folds.\n",
        "\n",
        "* **Observation**\n",
        " * *High Test Accuracy*: While k-NN achieves the highest test set accuracy (86.84%), its poor cross-validation mean accuracy (62.05%) indicates it might be overfitting the data.\n",
        " * *Low Stability*: The cross-validation results show that k-NN performs inconsistently across different data splits.\n",
        " * *Strengths*: Very low false positives (6), making it better for avoiding unnecessary follow-ups.\n",
        " * *Weaknesses*: The poor generalization ability (as shown by the low cross-validation mean accuracy) is a significant drawback.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "*  k-NN performs well in precision and test accuracy but struggles with generalization due to poor cross-validation accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "5lMRHdZI0prV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classification"
      ],
      "metadata": {
        "id": "n-yJQS--hM98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I'll use the Decision Tree Classifier to model the problem at hand."
      ],
      "metadata": {
        "id": "nQkNFW7IhotH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "U0wZRccvh5lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now l'll be implementing confusion matrix for Decision tree classsifier  after that implementing cross validation step.\n",
        "\n",
        "\n",
        "\n",
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "XDF-T8zEh83A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "GgpuFUEYiUAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[24 , 9]\n",
        "\n",
        " [ 6 , 37]]\n",
        "\n",
        "0.8026315789473685\n",
        "\n",
        "\n",
        "The above matrix indicates that\n",
        "1. True Negatives\t24\n",
        "2. False Positives\t9\n",
        "3. False Negatives\t6\n",
        "4. True Positives\t37\n",
        "\n",
        "* *Key insights*:\n",
        "\n",
        " *  True Negatives (24): The Decision Tree correctly identifies 24 non-disease cases.\n",
        " *  False Positives (9): Same as SVM and k-NN, meaning the model predicts more \"Disease\" cases than actually exist (i.e., a higher false positive rate).\n",
        " *  False Negatives (6): This is higher than both k-NN and SVM (4 and 2, respectively), meaning the Decision Tree misses more true disease cases.\n",
        " *  True Positives (37): Correctly predicts \"Disease\" in 37 cases.\n",
        "\n",
        " **Cross Validation**"
      ],
      "metadata": {
        "id": "0K5LoELPiaOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n",
        "print(\"Standard Deviation:\", scores.std())"
      ],
      "metadata": {
        "id": "mkQv3Tp1lA35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation Scores: [0.7704918  0.85245902 0.73770492 0.73333333 0.66666667]\n",
        "\n",
        "Mean Accuracy: 0.7521311475409835\n",
        "\n",
        "Standard Deviation: 0.060445754781489364\n",
        " 1. Mean Accuracy (75.21%): Higher than Random Forest (79.21%) but lower than k-NN (62.05%) and SVM (82.83%).\n",
        " 2. Standard Deviation (6.04%): Similar to Random Forest (6.13%) and k-NN (6.76%), indicating some variability in performance across folds.\n",
        "\n",
        "**Observations**\n",
        " * *False Negatives*: The Decision Tree has more false negatives (6) than the other models, which might be a concern in a medical diagnosis setting where missing positive cases is crucial.\n",
        " * *Test Set Accuracy*: While the test set accuracy (80.26%) is decent, it is still below that of k-NN (86.84%) and SVM (85.53%).\n",
        " * *Stability*: The Decision Tree’s cross-validation stability (75.21%) is better than k-NN's but slightly worse than Random Forest's.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "* Decision Tree is the weakest model in this comparison due to its lower accuracy and recall."
      ],
      "metadata": {
        "id": "YmVjI7mYlRzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classification"
      ],
      "metadata": {
        "id": "8zwfV145vxnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I'll use the ensemble method, Random Forest Classifier."
      ],
      "metadata": {
        "id": "Uc0ScnF0wEOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "r45yecukwPva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now l'll be implementing confusion matrix for Random Forest classsifier after that implementing cross validation step.\n",
        "\n",
        "\n",
        "\n",
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "zOvdqjTZwTOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "p8-QdVD2wmWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[24 , 9]\n",
        "\n",
        " [ 4 ,39]]\n",
        "\n",
        "0.8289473684210527\n",
        "\n",
        "The above metrix indicates that ,\n",
        "\n",
        "1. True Negatives\t24\n",
        "2. False Positives\t9\n",
        "3. False Negatives\t4\n",
        "4. True Positives\t39\n",
        "\n",
        "*Key insights*:\n",
        "\n",
        "   * False Positives (9): Incorrectly predicted as \"Disease\" when there is no disease.\n",
        "   * False Negatives (4): Cases where the model missed predicting \"Disease\" correctly (a higher number than SVM's 2 false negatives).\n",
        "\n",
        "\n",
        "**cross validation**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7SioPutFwq5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean Accuracy:\", scores.mean())\n",
        "print(\"Standard Deviation:\", scores.std())"
      ],
      "metadata": {
        "id": "dgR-UfMPxeZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation Scores: [0.81967213 0.85245902 0.70491803 0.85       0.73333333]\n",
        "\n",
        "Mean Accuracy: 0.7920765027322405\n",
        "\n",
        "Standard Deviation: 0.061335237887616995\n",
        "\n",
        "1. Mean Accuracy (79.21%): Lower than both the test accuracy and the SVM model's cross-validation mean accuracy (82.83%).\n",
        "2. Standard Deviation (6.13%): Higher than the SVM model's standard deviation (4.29%), indicating the Random Forest model is less stable across different folds.\n",
        "\n",
        "**Observations**:\n",
        "* *Performance*: The Random Forest model is solid but slightly underperforms compared to the SVM model, both in test accuracy and cross-validation scores.\n",
        "* *False Negatives*: Random Forest has higher false negatives (4) compared to SVM (2), which could be crucial in a medical context where missing actual cases is more severe than false alarms.\n",
        "* *Model Stability*: The higher standard deviation (6.13%) compared to SVM (4.29%) suggests the Random Forest model may be more sensitive to changes in the dataset.\n",
        "\n",
        "**Conclusion:**\n",
        "  * Random Forest provides a good trade-off between accuracy and recall but is slightly weaker than SVM in generalization."
      ],
      "metadata": {
        "id": "YNUvHGjvxrIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conclusion\n"
      ],
      "metadata": {
        "id": "3jRb6-iZyvn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I applied Machine Learning techniques to predict whether a person is suffering from heart disease. After importing the dataset, I performed data preprocessing, including data visualization, generating dummy variables for categorical features, and scaling the numerical features for uniformity.\n",
        "\n",
        "I implemented and compared the performance of four Machine Learning algorithms: k-Nearest Neighbors (k-NN), Support Vector Machine (SVM), Decision Tree Classifier, and Random Forest Classifier. I fine-tuned the hyperparameters for each model to optimize their performance.\n",
        "\n",
        "Among the models, the k-Nearest Neighbors Classifier achieved the highest test accuracy of 87%, using 8 nearest neighbors. The Support Vector Machine showed strong generalization capability with the highest cross-validation score, making it another robust choice.\n",
        "\n",
        "This project highlights the potential of Machine Learning in accurately predicting heart disease, contributing to more efficient and early diagnosis in the medical field.\n",
        "\n"
      ],
      "metadata": {
        "id": "HquiUKQy07YM"
      }
    }
  ]
}